Title:
Hypothesis: Reducing Vocabulary Complexity in Large Language Models via Simple-Word Compound Substitution

Abstract:
Large Language Models (LLMs) exhibit strong performance across a range of language tasks, but their extensive vocabulary sizes—often exceeding 100,000 tokens—contribute significantly to computational and memory costs. This paper explores a hypothesis: that replacing complex or low-frequency words with semantically equivalent compounds made from a fixed set of common words may reduce vocabulary size while preserving or even enhancing expressivity. By limiting the core vocabulary to around 20,000 frequently used words and constructing compounds from them, it may be possible to build more efficient, interpretable, and generalizable LLMs. While this idea remains untested, we outline its potential benefits, implementation strategies, and the challenges that must be addressed in future empirical studies.

arXiv Endorsement needed for cs.CL
